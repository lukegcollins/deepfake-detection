{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continual Learning Strategy Experiments\n",
    "\n",
    "**Import Packages**  \n",
    "The following cell will import the required packages, and print the their current version, and indicate how many GPU's are connected to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Dependencies | Double Click to Expand/Collapse\n",
    "# Author: Luke Collins \n",
    "# Date Created: 2023-09-28\n",
    "# Date Modified: 2023-09-28\n",
    "# Description: This file contains the code used in the publication \"XX\" to select the base_model for this study.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pydot\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, f1_score, auc\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.applications import Xception, InceptionV3, ResNet50, VGG16, VGG19 \n",
    "from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess_input\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_v3_preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess_input\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg19_preprocess_input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "print(\"Printing versions of libraries used:\")\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Seaborn:\", sns.__version__)\n",
    "print(\"Pydot:\", pydot.__version__)\n",
    "print(\"Matplotlib:\", mpl.__version__)\n",
    "print(\"Scikit-learn version:\", sk.__version__)\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## **Notebook Configuration Cell**  \n",
    "The following cell should always be run directly after the packages are imported, this is where you will configure directories to suit your system if reproducing this experiment at home. Do not proceed if your cells output produces and `false` for any of the directories.\n",
    "\n",
    "**Warning**   \n",
    "Before altering path, if you wish the use a different dataset on a specific test, we recommend change the `experiment_train_directory` or `experiment_test_directory` on lines 5-6 of the experiment cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Configuration File | Double Click to Expand/Collapse\n",
    "# Load Directories\n",
    "train_directory = \"../datasets/train-20\"\n",
    "test_directory = \"../datasets/test-20\"\n",
    "experiment_base_directory = \"../experiments/continual-learning-strategy\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(experiment_base_directory, exist_ok=True)\n",
    "\n",
    "# Check if directories exist\n",
    "print(\"Train directory exists:\", os.path.isdir(train_directory))\n",
    "print(\"Test directory exists:\", os.path.isdir(test_directory))\n",
    "print(\"Experiment directory exists:\", os.path.isdir(experiment_base_directory))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## **Dataset Creation Utilities**  \n",
    "The following cell will define the functions required to create the training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Dataset Creation Utilities | Double Click to Expand/Collapse\n",
    "def create_train_val_datasets(\n",
    "    preprocessing_model,\n",
    "    input_dir,\n",
    "    img_height=299,\n",
    "    img_width=299,\n",
    "    batch_size=32,\n",
    "    augment_data=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates train and validation datasets from images in the input directory.\n",
    "\n",
    "    Args:\n",
    "        preprocessing_model (function): The preprocessing function to apply to the images.\n",
    "        input_dir (str): The path to the input directory containing 'real' and 'fake' subdirectories.\n",
    "        img_height (int): The height of the input images. Default is 299.\n",
    "        img_width (int): The width of the input images. Default is 299.\n",
    "        batch_size (int): The batch size for the data generators. Default is 32.\n",
    "    Returns:\n",
    "        tf.data.Dataset, tf.data.Dataset: The train and validation datasets.\n",
    "    \"\"\"\n",
    "    normalDatagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocessing_model,\n",
    "        validation_split=0.2                # Create a 80/20 train/validation split\n",
    "    )\n",
    "\n",
    "    augmentDatagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocessing_model,\n",
    "            rotation_range=10,              # Random rotation within +/- 10 degrees\n",
    "            width_shift_range=0.1,          # Random horizontal shift within +/- 10% of image width\n",
    "            height_shift_range=0.1,         # Random vertical shift within +/- 10% of image height\n",
    "            shear_range=0.1,                # Random shearing\n",
    "            zoom_range=0.1,                 # Random zooming\n",
    "            horizontal_flip=True,           # Horizontal flipping\n",
    "            brightness_range=[0.8, 1.2],    # Random brightness adjustment within the given range\n",
    "            validation_split=0.2            # Create a 80/20 train/validation split\n",
    "    )\n",
    "\n",
    "\n",
    "    datagen = augmentDatagen if augment_data else normalDatagen\n",
    "    train_gen = datagen.flow_from_directory(\n",
    "        input_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"binary\",\n",
    "        subset=\"training\",\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_gen = normalDatagen.flow_from_directory(\n",
    "        input_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"binary\",\n",
    "        subset=\"validation\",\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    return train_gen, val_gen\n",
    "\n",
    "def create_test_dataset(\n",
    "    preprocessing_model,\n",
    "    input_dir,\n",
    "    img_height=299,\n",
    "    img_width=299,\n",
    "    batch_size=32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a test dataset from images in the input directory.\n",
    "\n",
    "    Args:\n",
    "        preprocessing_model (function): The preprocessing function to apply to the images.\n",
    "        input_dir (str): The path to the input directory containing 'real' and 'fake' subdirectories..\n",
    "        img_height (int): The height of the input images. Default is 299.\n",
    "        img_width (int): The width of the input images. Default is 299.\n",
    "        batch_size (int): The batch size for the data generators. Default is 32.\n",
    "        save_dataset (bool): Whether to save the dataset as a TFRecord file. Default is True.\n",
    "    Returns:\n",
    "        tf.data.Dataset: The test dataset.\n",
    "    \"\"\"\n",
    "    datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocessing_model\n",
    "    )\n",
    "\n",
    "    return datagen.flow_from_directory(\n",
    "        input_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"binary\",\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## **Plotting Utilities**  \n",
    "The following cell will define the functions required to plot model accuracy on training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Plot/Graphic Creation Utilities | Double Click to Expand/Collapse\n",
    "def create_accuracy_plots(folder_path):\n",
    "    \"\"\"\n",
    "    Creates accuracy plots for each epoch report in the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing the epoch reports.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get a list of all CSV files in the folder\n",
    "    epoch_reports = [file for file in os.listdir(folder_path) if file.endswith('epochs_report.csv')]\n",
    "\n",
    "    # Create the accuracy_plots directory if it doesn't exist\n",
    "    accuracy_plots_path = os.path.join(folder_path, 'accuracy_plots')\n",
    "    os.makedirs(accuracy_plots_path, exist_ok=True)\n",
    "\n",
    "    # Cycle through the epoch reports and create plots\n",
    "    for report in epoch_reports:\n",
    "        # Read the epoch history CSV file\n",
    "        report_path = os.path.join(folder_path, report)\n",
    "        df = pd.read_csv(report_path)\n",
    "\n",
    "        # Get the number of epochs\n",
    "        epochs = range(1, len(df) + 1)\n",
    "\n",
    "        # Extract the relevant columns\n",
    "        train_accuracy = df['binary_accuracy']\n",
    "        val_accuracy = df['val_binary_accuracy']\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(epochs, train_accuracy, label='Train Accuracy')\n",
    "        plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "\n",
    "        # Set plot title and labels\n",
    "        plot_name = report.split('_')[0] + '_accuracy_plot.png'\n",
    "        plt.title(f'Accuracy History - {plot_name}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "\n",
    "        # Set plot legend\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(accuracy_plots_path, plot_name)\n",
    "        plt.savefig(plot_path)\n",
    "\n",
    "        # Display the plot (optional)\n",
    "        plt.show()\n",
    "\n",
    "def create_all_metrics_plots(folder_path):\n",
    "    \"\"\"\n",
    "    Creates plots for various metrics (accuracy, precision, recall) for each epoch report in the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing the epoch reports.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get a list of all CSV files in the folder\n",
    "    epoch_reports = [file for file in os.listdir(folder_path) if file.endswith('epochs_report.csv')]\n",
    "\n",
    "    # Create the accuracy_plots directory if it doesn't exist\n",
    "    accuracy_plots_path = os.path.join(folder_path, 'accuracy_plots')\n",
    "    os.makedirs(accuracy_plots_path, exist_ok=True)\n",
    "\n",
    "    # Cycle through the epoch reports and create plots\n",
    "    for report in epoch_reports:\n",
    "        # Read the epoch history CSV file\n",
    "        report_path = os.path.join(folder_path, report)\n",
    "        df = pd.read_csv(report_path)\n",
    "\n",
    "        # Get the number of epochs\n",
    "        epochs = range(1, len(df) + 1)\n",
    "\n",
    "        # Extract the relevant columns\n",
    "        train_accuracy = df['binary_accuracy']\n",
    "        train_precision = df['precision']\n",
    "        train_recall = df['recall']\n",
    "        val_accuracy = df['val_binary_accuracy']\n",
    "        val_precision = df['val_precision']\n",
    "        val_recall = df['val_recall']\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(epochs, train_accuracy, label='Train Accuracy')\n",
    "        plt.plot(epochs, train_precision, label='Train Precision')\n",
    "        plt.plot(epochs, train_recall, label='Train Recall')\n",
    "        plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "        plt.plot(epochs, val_precision, label='Validation Precision')\n",
    "        plt.plot(epochs, val_recall, label='Validation Recall')\n",
    "\n",
    "        # Set plot title and labels\n",
    "        plot_name = report.split('_')[0] + '_metrics_plot.png'\n",
    "        plt.title(f'Metrics History - {plot_name}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Metrics')\n",
    "\n",
    "        # Set plot legend\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(accuracy_plots_path, plot_name)\n",
    "        plt.savefig(plot_path)\n",
    "\n",
    "        # Display the plot (optional)\n",
    "        plt.show()\n",
    "        \n",
    "def create_accuracy_loss_plots(folder_path, experimentId):\n",
    "    \"\"\"\n",
    "    Creates plots for various metrics (accuracy, loss) for each epoch report in the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing the epoch reports.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get a list of all CSV files in the folder\n",
    "    epoch_reports = [file for file in os.listdir(folder_path) if file.endswith('epochs_report.csv')]\n",
    "\n",
    "    # Create the accuracy_plots directory if it doesn't exist\n",
    "    accuracy_plots_path = os.path.join(folder_path, 'accuracy_plots')\n",
    "    os.makedirs(accuracy_plots_path, exist_ok=True)\n",
    "\n",
    "    # Cycle through the epoch reports and create plots\n",
    "    for report in epoch_reports:\n",
    "        # Read the epoch history CSV file\n",
    "        report_path = os.path.join(folder_path, report)\n",
    "        df = pd.read_csv(report_path)\n",
    "\n",
    "        # Create the figure and subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # Plot for the first subplot\n",
    "        ax1.plot(df['binary_accuracy'], label='Training Accuracy')\n",
    "        ax1.plot(df['val_binary_accuracy'], label='Validation Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title('Training and Validation Accuracy')\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot for the second subplot\n",
    "        ax2.plot(df['loss'], label='Training Loss')\n",
    "        ax2.plot(df['val_loss'], label='Validation Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Training and Validation Loss')\n",
    "        ax2.legend()\n",
    "\n",
    "        # Add a title that spans over the two plots\n",
    "        plot_name = report.split('_')[0] + '_metrics_plot.png'\n",
    "        fig.suptitle(f'{experimentID}\\nTraining History for {plot_name}\\n', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Adjust the spacing between subplots and the suptitle\n",
    "        plt.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(accuracy_plots_path, f'{plot_name}_report')\n",
    "        plt.savefig(plot_path)\n",
    "\n",
    "        # Display the plot (optional)\n",
    "        plt.show()      \n",
    "\n",
    "def create_model_image(model, output_dir, name = 'model', extension = 'png'):\n",
    "    \"\"\"\n",
    "    Creates an image of a TensorFlow model's structure and saves it to the specified directory.\n",
    "\n",
    "    Args:\n",
    "        model: The TensorFlow model to visualize.\n",
    "        output_dir: The directory where the model image should be saved.\n",
    "        name: The name of the model image file. Default is 'model'.\n",
    "        extension: The file extension for the model image file. Default is 'png'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if extension not in ['png', 'svg', 'pdf', 'jpg']:\n",
    "        throw(f'Invalid file extension: {extension}')\n",
    "        \n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Construct the full path for the output file\n",
    "    output_file = os.path.join(output_dir, f'{name}.{extension}')\n",
    "\n",
    "    # Generate and save the model image\n",
    "    plot_model(model, to_file=output_file, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Elastic Weight Consolidation (EWC) class\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Elastic Weight Consolidation (EWC) class\n",
    "class EWC:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.ewc_losses = []\n",
    "\n",
    "    def compute_ewc_loss(self, prev_weights, fisher_matrix):\n",
    "        ewc_loss = 0.0\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            if layer.trainable_weights:\n",
    "                ewc_loss += tf.reduce_sum(fisher_matrix[i] * tf.square(layer.weights[0] - prev_weights[i]))\n",
    "        return ewc_loss\n",
    "\n",
    "    def register_ewc_losses(self, ewc_losses):\n",
    "        self.ewc_losses = ewc_losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Experiments\n",
    "---\n",
    "## Experiment #CLS-0001 | \n",
    "**Experiment ID:** #CLS-0001  \n",
    "**Experiment Description:** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment ID:CLS-0001\n",
    "experimentId = \"CLS-0001\"\n",
    "\n",
    "# Load Directories (Leave alone unless specifying a different dataset)\n",
    "experiment_train_directory = train_directory\n",
    "experiment_test_directory = test_directory\n",
    "experiment_directory = f\"{experiment_base_directory}/{experimentId}\"\n",
    "experiment_results_directory = f\"{experiment_directory}/results\"\n",
    "experiment_models_directory = f\"{experiment_directory}/models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(name=experiment_directory, exist_ok=True)\n",
    "os.makedirs(name=experiment_models_directory, exist_ok=True)\n",
    "os.makedirs(name=experiment_results_directory, exist_ok=True)\n",
    "\n",
    "# Declare lists for report generation\n",
    "accuracy_report = []\n",
    "\n",
    "# Declare Test Configurations\n",
    "test_configurations = [\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/FF++/train-1\"],\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/FF++/train-2\"],\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/FF++/train-3\"],\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/FF++/train-4\"],\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/FF++/train-5\"],\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/FF++/train-6\"],\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/FF++/train-7\"],\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/FF++/train-8\"],\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/FF++/train-9\"],\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/FF++/train-10\"],\n",
    "    \n",
    "]\n",
    "\n",
    "# Declare accuracy_test list\n",
    "accuracy_test = []\n",
    "\n",
    "# Define Early Stopping Callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    mode='min', \n",
    "    verbose=1, \n",
    "    patience=8\n",
    ")\n",
    "\n",
    "# Define Reduce Learning Rate on Plateau Callback\n",
    "lr_reducer = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Set Loop Counter \n",
    "loop_counter = 0\n",
    "# Loop through all combinations of models and trainable layers\n",
    "for pretrained_model, preprocessing_function, batch_size, is_augmented, dataset_path in test_configurations:\n",
    "    loop_counter += 1\n",
    "    # Clear Session\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Setup Model Check Pointing\n",
    "    nameSuffix = 'augmented' if is_augmented == True else 'not_augmented'\n",
    "    model_checkpoint_vl = ModelCheckpoint(\n",
    "        filepath=f\"{experiment_models_directory}/{pretrained_model.__name__}_model_{nameSuffix}_vl.h5\",\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        save_best_only=True\n",
    "    )\n",
    "    \n",
    "    model_checkpoint_ba = ModelCheckpoint(\n",
    "        filepath=f\"{experiment_models_directory}/{pretrained_model.__name__}_model_{nameSuffix}_ba.h5\",\n",
    "        monitor='val_binary_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        save_best_only=True\n",
    "    )\n",
    "    \n",
    "    # Create a dataset and preprocess images to suit base model\n",
    "    train_ds, val_ds = create_train_val_datasets(preprocessing_function, dataset_path, batch_size=batch_size, augment_data=is_augmented)\n",
    "    test_ds = create_test_dataset(preprocessing_function, experiment_test_directory, batch_size=batch_size)\n",
    "\n",
    "    # Create Input Layer\n",
    "    inputs = Input(shape=(299, 299, 3))\n",
    "\n",
    "    #####################################################################################################################################\n",
    "    # Load Previous Model and Weights\n",
    "    #####################################################################################################################################\n",
    "\n",
    "    # Load Model\n",
    "    model = load_model(\n",
    "        f\"/home/luke/Development/deepfake-detection/deepfake-detection-code/experiments/base-model-selection/BMS-0004/models/ResNet50_finetuned_model_augmented.h5\",\n",
    "    )\n",
    "    \n",
    "    # Create Input Layer\n",
    "    inputs = Input(shape=(299, 299, 3))\n",
    "\n",
    "    # Base model is the second layer in the model\n",
    "    base_model = model.layers[1]\n",
    "\n",
    "    # Make Base Model Trainable\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Freeze the base model (do not train everything but last 15 layers)\n",
    "    for layer in base_model.layers[:-26]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Hookup Previous Model Layers to New Model\n",
    "    inputs = keras.Input(shape=(299, 299, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # Traverse the layers and connect them\n",
    "    for layer in model.layers[2:]:\n",
    "        x = layer(x)\n",
    "    \n",
    "    # Define the model\n",
    "    model = keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-4),  \n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(), Precision(), Recall(), AUC()]\n",
    "    )\n",
    "\n",
    "    # Initialize EWC\n",
    "    ewc = EWC(model)\n",
    "\n",
    "    #####################################################################################################################################\n",
    "    # Begin Fine Tuning Entire Model with EWC\n",
    "    #####################################################################################################################################\n",
    "\n",
    "    # Start Timer to measure processing time \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        steps_per_epoch=train_ds.samples // train_ds.batch_size,\n",
    "        epochs=15, \n",
    "        validation_data=test_ds,\n",
    "        validation_steps=test_ds.samples // test_ds.batch_size,\n",
    "        callbacks=[early_stopping, lr_reducer, model_checkpoint_vl, model_checkpoint_ba]\n",
    "    )\n",
    "\n",
    "    # Calculate Fisher Matrix\n",
    "    fisher_matrix = []\n",
    "    for layer in model.layers:\n",
    "        if layer.trainable_weights:\n",
    "            fisher_matrix.append(ewc.compute_fisher_matrix(layer))\n",
    "\n",
    "    # Compute EWC Loss\n",
    "    ewc_loss = ewc.compute_ewc_loss(prev_weights, fisher_matrix)\n",
    "    \n",
    "    # Add EWC Loss to the model's total loss\n",
    "    model.add_loss(ewc_loss)\n",
    "\n",
    "    # End Timer and calculate processing time\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate = model.evaluate(test_ds, steps=test_ds.samples // test_ds.batch_size, verbose=1)\n",
    "\n",
    "    # Append results to accuracy_test list\n",
    "    history_key = list(history.history.keys()) # As names of metric names can be dynamic, we need to get the keys of the history object\n",
    "    accuracy_test.append({\n",
    "        'Base Model': pretrained_model.__name__,\n",
    "        'duration': round(processing_time, 3),\n",
    "        'test_accuracy': round(evaluate[1], 3),\n",
    "        'test_precision': round(evaluate[2], 3),\n",
    "        'test_recall': round(evaluate[3], 3),\n",
    "        'test_loss': round(evaluate[0], 3),\n",
    "        'train_accuracy': round(history.history[history_key[1]][-1], 3),\n",
    "        'train_precision': round(history.history[history_key[2]][-1], 3),\n",
    "        'train_recall': round(history.history[history_key[3]][-1], 3),\n",
    "        'train_auc': round(history.history[history_key[4]][-1], 3),\n",
    "        'train_loss': round(history.history[history_key[0]][-1], 3),\n",
    "        'val_accuracy': round(history.history[history_key[6]][-1], 3),\n",
    "        'val_precision': round(history.history[history_key[7]][-1], 3),\n",
    "        'val_recall': round(history.history[history_key[8]][-1], 3),\n",
    "        'val_auc': round(history.history[history_key[9]][-1], 3),\n",
    "        'val_loss': round(history.history[history_key[5]][-1], 3)\n",
    "    })\n",
    "\n",
    "    epochs_report = pd.DataFrame(history.history)\n",
    "    epochs_report.to_csv(f\"{experiment_results_directory}/{pretrained_model.__name__}_{loop_counter}_epochs_report.csv\", index=False)\n",
    "\n",
    "    model.save(f\"{experiment_models_directory}/{pretrained_model.__name__}_finetuned_model_{nameSuffix}.h5\")\n",
    "\n",
    "# Create DataFrame with accuracy score and export as .csv\n",
    "accuracy_test_results = pd.DataFrame(accuracy_test)\n",
    "accuracy_test_results.to_csv(f'{experiment_results_directory}/accuracy_test_results.csv', index=False)\n",
    "\n",
    "# Generate graphs from epochs_reports.csv and output to results folder.\n",
    "create_accuracy_plots(f\"{experiment_results_directory}\")\n",
    "create_all_metrics_plots(f\"{experiment_results_directory}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment #CLS-0002 | \n",
    "**Experiment ID:** #TMS-0002  \n",
    "**Experiment Description:** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment ID:CLS-0002\n",
    "experimentId = \"CLS-0002\"\n",
    "\n",
    "# Load Directories (Leave alone unless specifying a different dataset)\n",
    "experiment_train_directory = train_directory\n",
    "experiment_test_directory = test_directory\n",
    "experiment_directory = f\"{experiment_base_directory}/{experimentId}\"\n",
    "experiment_results_directory = f\"{experiment_directory}/results\"\n",
    "experiment_models_directory = f\"{experiment_directory}/models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(name=experiment_directory, exist_ok=True)\n",
    "os.makedirs(name=experiment_models_directory, exist_ok=True)\n",
    "os.makedirs(name=experiment_results_directory, exist_ok=True)\n",
    "\n",
    "# Declare lists for report generation\n",
    "accuracy_report = []\n",
    "\n",
    "# Declare Test Configurations\n",
    "test_configurations = [\n",
    "    [ResNet50, resnet50_preprocess_input, 16, True, \"../datasets/Celeb-DF/train-10/\"],\n",
    "    \n",
    "]\n",
    "\n",
    "# Declare accuracy_test list\n",
    "accuracy_test = []\n",
    "\n",
    "# Define Early Stopping Callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    mode='min', \n",
    "    verbose=1, \n",
    "    patience=8\n",
    ")\n",
    "\n",
    "# Define Reduce Learning Rate on Plateau Callback\n",
    "lr_reducer = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Set Loop Counter \n",
    "loop_counter = 0\n",
    "# Loop through all combinations of models and trainable layers\n",
    "for pretrained_model, preprocessing_function, batch_size, is_augmented, dataset_path in test_configurations:\n",
    "    loop_counter += 1\n",
    "    # Clear Session\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Setup Model Check Pointing\n",
    "    nameSuffix = 'augmented' if is_augmented == True else 'not_augmented'\n",
    "    model_checkpoint_vl = ModelCheckpoint(\n",
    "        filepath=f\"{experiment_models_directory}/{pretrained_model.__name__}_model_{nameSuffix}_vl.h5\",\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        save_best_only=True\n",
    "    )\n",
    "    \n",
    "    model_checkpoint_ba = ModelCheckpoint(\n",
    "        filepath=f\"{experiment_models_directory}/{pretrained_model.__name__}_model_{nameSuffix}_ba.h5\",\n",
    "        monitor='val_binary_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        save_best_only=True\n",
    "    )\n",
    "    \n",
    "    # Create a dataset and preprocess images to suit base model\n",
    "    train_ds, val_ds = create_train_val_datasets(preprocessing_function, dataset_path, batch_size=batch_size, augment_data=is_augmented)\n",
    "    test_ds = create_test_dataset(preprocessing_function, experiment_test_directory, batch_size=batch_size)\n",
    "\n",
    "    # Create Input Layer\n",
    "    inputs = Input(shape=(299, 299, 3))\n",
    "\n",
    "    #####################################################################################################################################\n",
    "    # Load Previous Model and Weights\n",
    "    #####################################################################################################################################\n",
    "\n",
    "    # Load Model\n",
    "    model = load_model(\n",
    "        f\"/home/luke/Development/deepfake-detection/deepfake-detection-code/experiments/base-model-selection/BMS-0004/models/ResNet50_finetuned_model_augmented.h5\",\n",
    "    )\n",
    "    \n",
    "    # Create Input Layer\n",
    "    inputs = Input(shape=(299, 299, 3))\n",
    "\n",
    "    # Base model is the second layer in the model\n",
    "    base_model = model.layers[1]\n",
    "\n",
    "    # Make Base Model Trainable\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Freeze the base model (do not train everything but last 15 layers)\n",
    "    for layer in base_model.layers[:-26]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Hookup Previous Model Layers to New Model\n",
    "    inputs = keras.Input(shape=(299, 299, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # Traverse the layers and connect them\n",
    "    for layer in model.layers[2:]:\n",
    "        x = layer(x)\n",
    "    \n",
    "    # Define the model\n",
    "    model = keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-4),  \n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[BinaryAccuracy(), Precision(), Recall(), AUC()]\n",
    "    )\n",
    "\n",
    "    # Initialize EWC\n",
    "    ewc = EWC(model)\n",
    "\n",
    "    #####################################################################################################################################\n",
    "    # Begin Fine Tuning Entire Model with EWC\n",
    "    #####################################################################################################################################\n",
    "\n",
    "    # Start Timer to measure processing time \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        steps_per_epoch=train_ds.samples // train_ds.batch_size,\n",
    "        epochs=15, \n",
    "        validation_data=test_ds,\n",
    "        validation_steps=test_ds.samples // test_ds.batch_size,\n",
    "        callbacks=[early_stopping, lr_reducer, model_checkpoint_vl, model_checkpoint_ba]\n",
    "    )\n",
    "\n",
    "    # Calculate Fisher Matrix\n",
    "    fisher_matrix = []\n",
    "    for layer in model.layers:\n",
    "        if layer.trainable_weights:\n",
    "            fisher_matrix.append(ewc.compute_fisher_matrix(layer))\n",
    "\n",
    "    # Compute EWC Loss\n",
    "    ewc_loss = ewc.compute_ewc_loss(prev_weights, fisher_matrix)\n",
    "    \n",
    "    # Add EWC Loss to the model's total loss\n",
    "    model.add_loss(ewc_loss)\n",
    "\n",
    "    # End Timer and calculate processing time\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate = model.evaluate(test_ds, steps=test_ds.samples // test_ds.batch_size, verbose=1)\n",
    "\n",
    "    # Append results to accuracy_test list\n",
    "    history_key = list(history.history.keys()) # As names of metric names can be dynamic, we need to get the keys of the history object\n",
    "    accuracy_test.append({\n",
    "        'Base Model': pretrained_model.__name__,\n",
    "        'duration': round(processing_time, 3),\n",
    "        'test_accuracy': round(evaluate[1], 3),\n",
    "        'test_precision': round(evaluate[2], 3),\n",
    "        'test_recall': round(evaluate[3], 3),\n",
    "        'test_loss': round(evaluate[0], 3),\n",
    "        'train_accuracy': round(history.history[history_key[1]][-1], 3),\n",
    "        'train_precision': round(history.history[history_key[2]][-1], 3),\n",
    "        'train_recall': round(history.history[history_key[3]][-1], 3),\n",
    "        'train_auc': round(history.history[history_key[4]][-1], 3),\n",
    "        'train_loss': round(history.history[history_key[0]][-1], 3),\n",
    "        'val_accuracy': round(history.history[history_key[6]][-1], 3),\n",
    "        'val_precision': round(history.history[history_key[7]][-1], 3),\n",
    "        'val_recall': round(history.history[history_key[8]][-1], 3),\n",
    "        'val_auc': round(history.history[history_key[9]][-1], 3),\n",
    "        'val_loss': round(history.history[history_key[5]][-1], 3)\n",
    "    })\n",
    "\n",
    "    epochs_report = pd.DataFrame(history.history)\n",
    "    epochs_report.to_csv(f\"{experiment_results_directory}/{pretrained_model.__name__}_{loop_counter}_epochs_report.csv\", index=False)\n",
    "\n",
    "    model.save(f\"{experiment_models_directory}/{pretrained_model.__name__}_finetuned_model_{nameSuffix}.h5\")\n",
    "\n",
    "# Create DataFrame with accuracy score and export as .csv\n",
    "accuracy_test_results = pd.DataFrame(accuracy_test)\n",
    "accuracy_test_results.to_csv(f'{experiment_results_directory}/accuracy_test_results.csv', index=False)\n",
    "\n",
    "# Generate graphs from epochs_reports.csv and output to results folder.\n",
    "create_accuracy_plots(f\"{experiment_results_directory}\")\n",
    "create_all_metrics_plots(f\"{experiment_results_directory}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
